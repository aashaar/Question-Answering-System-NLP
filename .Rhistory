############################## Most frequent terms ##############################
dt_matrix <- DocumentTermMatrix(plots =  plot_summaries, query = processedQuery)
frequent_matrix<- addmargins(dt_matrix, margin = 2)
frequent_matrix<- frequent_matrix[order(frequent_matrix[,15], decreasing = TRUE),]
top_fifty_frequentmatrix<- head(frequent_matrix[,c(1,15)], n = 50)
print(top_fifty_frequentmatrix)
######################### Word Cloud ####################################
wordTable<- head(frequent_matrix[,15], n = 150)
wordcloud(words = names(wordTable),
freq = as.numeric(wordTable),
scale = c(2, 1),
min.freq = 1)
############################## Search Results ##############################
results_df <- findResultant_DF(plots =  plot_summaries, query = processedQuery)
resultDF <- merge(results_df, movieNames, by = "doc")
names(resultDF) <- c("doc", "score")
printDF <- resultDF[order(resultDF$score, decreasing = TRUE), ]
printDF <- printDF[,c(1,2,4)]
print()
names(printDF) <- c("doc", "score", "Movie Title")
if(is.na(printDF$score[1])) {
print("Sorry, there are no matches for your query. Please try another one.")
} else {
print(printDF[0:10,], row.names = FALSE, right = FALSE, digits = 2)
}
input <- readline(prompt = "Press 'q' to exit the program OR Press any other key to continue ")
}
library(SnowballC)
library(wordcloud)
library(text2vec)
library(dplyr)
library(abind)
############################## Preprocess the text data ##############################
preprocess <- function(text){
corpus <- Corpus(VectorSource(text))
uniqueWords = content_transformer(function(x) { unique(strsplit(x, " ")[[1]]) })
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c("a",
"about","above","after","again","against","all","am","an","and","any","are","aren't","as","at","be",
"because","been","before","being","below","between","both","but","by","can't","cannot","could","couldn't",
"did","didn't","do","does","doesn't","doing","don't","down","during","each","few","for","from","further",
"had","hadn't","has","hasn't","have","haven't","having","he","he'd","he'll","he's","her","here","here's",
"hers","herself","him","himself","his","how","how's","i","i'd","i'll","i'm","i've","if","in","into","is",
"isn't","it","it's","its","itself","let's","me","more","most","mustn't","my","myself","no","nor","not",
"of","off","on","once","only","or","other","ought","our","ours","ourselves","out","over","own","same",
"shan't","she","she'd","she'll","she's","should","shouldn't","so","some","such","than","that","that's",
"the","their","theirs","them","themselves","then","there","there's","these","they","they'd","they'll",
"they're","they've","this","those","through","to","too","under","until","up","very","was","wasn't","we",
"we'd","we'll","we're","we've","were","weren't","what","what's","when","when's","where","where's","which",
"while","who","who's","whom","why","why's","with","won't","would",
"wouldn't","you","you'd","you'll","you're","you've","your","yours","yourself","yourselves"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
corpus[[1]]$content
}
############################## TF-IDF ##############################
get_TF_IDF <- function(plots, query) {
cat("Calculating tf-idf values .......")
dtm1 <- DocumentTermMatrix(plots, query)
matrix <- t(apply(dtm1, 1,
FUN = function(row) {TF_IDF_Weights(row)}))
colnames(matrix) <- colnames(dtm1)
matrix <- scale(matrix, center = FALSE, scale = sqrt(colSums(matrix^2)))
document_count <- length(plots$summary)
query_vector <- matrix[, document_count + 1]
matrix <- matrix[, 1:document_count]
document_scores <- t(query_vector) %*% matrix
data.frame(doc = plots$doc_id, score = t(document_scores))
}
# Function to get term document weights for each document
TF_IDF_Weights <- function(tfvectors) {
document_count <- length(tfvectors)
document_frequency <- length(tfvectors[tfvectors > 0])
tfidf_weights <- rep(0, length(tfvectors))
tfidf_weights[tfvectors > 0] <- tfvectors[tfvectors > 0] * log2(document_count/document_frequency)
return(tfidf_weights)
}
findResultant_DF <- function(plots, query) {
query_word_count <- sapply(strsplit(query, " "), length)
if(query_word_count == 0) {
stop("Query cannot be NULL")
} else if (query_word_count == 1) {
get_TF_IDF(plots =  plots, query = query)
} else {
findCosineSimilarity(plots =  plots, query = query)
}
}
DocumentTermMatrix <- function(plots, query) {
document <- VectorSource(c(plots$summary, query))
document$names <- c(plots$doc_id, "query")
corpus <- Corpus(document)
corpus_dtm <- TermDocumentMatrix(corpus)
colnames(corpus_dtm) <- c(plots$doc_id, "query")
#Removing unnecessary elements as matrix is too sparse
sparse_corpus <- removeSparseTerms(corpus_dtm, 0.99)
as.matrix(sparse_corpus)
}
############################## COSINE Similarity ##############################
findCosineSimilarity <- function(plots, query) {
cat("Calculating Cosine Similarities -->")
documents <- plots$summary
iterator1 <- itoken(c(documents, query), progressbar = FALSE)
vocab = create_vocabulary(iterator1) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 3)
vectorizer = vocab_vectorizer(vocab)
iterator2 <- itoken(documents, progressbar = FALSE )
dtm1 = create_dtm(iterator2, vectorizer)
iterator3 <- itoken(query, progressbar = FALSE)
dtm2 = create_dtm(iterator3, vectorizer)
cosineSim = sim2(dtm1, dtm2, method = "cosine", norm = "l2")
cosine_matrix <- as.matrix(cosineSim)
data.frame(doc = plots$doc_id, score = cosine_matrix)
}
############################## Get Data ##############################
#df1 <- read.table("http://utdallas.edu/~dxm172530/R/plot_summaries.txt", header = FALSE,
#                  sep="\t", stringsAsFactors = FALSE, quote="")
#movieNames <- read.csv("http://utdallas.edu/~dxm172530/R/movie.metadata.tsv", sep = "\t", header = FALSE)
#colnames(movieNames) <- c("doc", "FreebaseID", "MovieName", "release date", "revenue",
#                          "runtime", "languages", "countries", "genres")
#Preprocess the data :
plot_summaries <- data.frame(doc_id = df1[,1], summary = df1[,2])
summaries <- lapply(plot_summaries$summary, as.character)
plot_summaries$summary <- lapply(summaries, preprocess)
require(tm)
library(SnowballC)
library(wordcloud)
library(text2vec)
library(dplyr)
library(abind)
############################## Preprocess the text data ##############################
preprocess <- function(text){
corpus <- Corpus(VectorSource(text))
uniqueWords = content_transformer(function(x) { unique(strsplit(x, " ")[[1]]) })
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c("a",
"about","above","after","again","against","all","am","an","and","any","are","aren't","as","at","be",
"because","been","before","being","below","between","both","but","by","can't","cannot","could","couldn't",
"did","didn't","do","does","doesn't","doing","don't","down","during","each","few","for","from","further",
"had","hadn't","has","hasn't","have","haven't","having","he","he'd","he'll","he's","her","here","here's",
"hers","herself","him","himself","his","how","how's","i","i'd","i'll","i'm","i've","if","in","into","is",
"isn't","it","it's","its","itself","let's","me","more","most","mustn't","my","myself","no","nor","not",
"of","off","on","once","only","or","other","ought","our","ours","ourselves","out","over","own","same",
"shan't","she","she'd","she'll","she's","should","shouldn't","so","some","such","than","that","that's",
"the","their","theirs","them","themselves","then","there","there's","these","they","they'd","they'll",
"they're","they've","this","those","through","to","too","under","until","up","very","was","wasn't","we",
"we'd","we'll","we're","we've","were","weren't","what","what's","when","when's","where","where's","which",
"while","who","who's","whom","why","why's","with","won't","would",
"wouldn't","you","you'd","you'll","you're","you've","your","yours","yourself","yourselves"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
corpus[[1]]$content
}
############################## TF-IDF ##############################
get_TF_IDF <- function(plots, query) {
cat("Calculating tf-idf values .......")
dtm1 <- DocumentTermMatrix(plots, query)
matrix <- t(apply(dtm1, 1,
FUN = function(row) {TF_IDF_Weights(row)}))
colnames(matrix) <- colnames(dtm1)
matrix <- scale(matrix, center = FALSE, scale = sqrt(colSums(matrix^2)))
document_count <- length(plots$summary)
query_vector <- matrix[, document_count + 1]
matrix <- matrix[, 1:document_count]
document_scores <- t(query_vector) %*% matrix
data.frame(doc = plots$doc_id, score = t(document_scores))
}
# Function to get term document weights for each document
TF_IDF_Weights <- function(tfvectors) {
document_count <- length(tfvectors)
document_frequency <- length(tfvectors[tfvectors > 0])
tfidf_weights <- rep(0, length(tfvectors))
tfidf_weights[tfvectors > 0] <- tfvectors[tfvectors > 0] * log2(document_count/document_frequency)
return(tfidf_weights)
}
findResultant_DF <- function(plots, query) {
query_word_count <- sapply(strsplit(query, " "), length)
if(query_word_count == 0) {
stop("Query cannot be NULL")
} else if (query_word_count == 1) {
get_TF_IDF(plots =  plots, query = query)
} else {
findCosineSimilarity(plots =  plots, query = query)
}
}
DocumentTermMatrix <- function(plots, query) {
document <- VectorSource(c(plots$summary, query))
document$names <- c(plots$doc_id, "query")
corpus <- Corpus(document)
corpus_dtm <- TermDocumentMatrix(corpus)
colnames(corpus_dtm) <- c(plots$doc_id, "query")
#Removing unnecessary elements as matrix is too sparse
sparse_corpus <- removeSparseTerms(corpus_dtm, 0.99)
as.matrix(sparse_corpus)
}
############################## COSINE Similarity ##############################
findCosineSimilarity <- function(plots, query) {
cat("Calculating Cosine Similarities -->")
documents <- plots$summary
iterator1 <- itoken(c(documents, query), progressbar = FALSE)
vocab = create_vocabulary(iterator1) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 3)
vectorizer = vocab_vectorizer(vocab)
iterator2 <- itoken(documents, progressbar = FALSE )
dtm1 = create_dtm(iterator2, vectorizer)
iterator3 <- itoken(query, progressbar = FALSE)
dtm2 = create_dtm(iterator3, vectorizer)
cosineSim = sim2(dtm1, dtm2, method = "cosine", norm = "l2")
cosine_matrix <- as.matrix(cosineSim)
data.frame(doc = plots$doc_id, score = cosine_matrix)
}
############################## Get Data ##############################
#df1 <- read.table("http://utdallas.edu/~dxm172530/R/plot_summaries.txt", header = FALSE,
#                  sep="\t", stringsAsFactors = FALSE, quote="")
#movieNames <- read.csv("http://utdallas.edu/~dxm172530/R/movie.metadata.tsv", sep = "\t", header = FALSE)
#colnames(movieNames) <- c("doc", "FreebaseID", "MovieName", "release date", "revenue",
#                          "runtime", "languages", "countries", "genres")
#Preprocess the data :
plot_summaries <- data.frame(doc_id = df1[,1], summary = df1[,2])
summaries <- lapply(plot_summaries$summary, as.character)
plot_summaries$summary <- lapply(summaries, preprocess)
plot_summaries <- data.frame(lapply(plot_summaries, as.character), stringsAsFactors = FALSE)
input <- ''
while(input != 'q') {
query <- readline(prompt="Enter your query: ")
processedQuery <- preprocess(query)
############################## Most frequent terms ##############################
dt_matrix <- DocumentTermMatrix(plots =  plot_summaries, query = processedQuery)
frequent_matrix<- addmargins(dt_matrix, margin = 2)
frequent_matrix<- frequent_matrix[order(frequent_matrix[,15], decreasing = TRUE),]
top_fifty_frequentmatrix<- head(frequent_matrix[,c(1,15)], n = 50)
print(top_fifty_frequentmatrix)
######################### Word Cloud ####################################
wordTable<- head(frequent_matrix[,15], n = 150)
wordcloud(words = names(wordTable),
freq = as.numeric(wordTable),
scale = c(2, 1),
min.freq = 1)
############################## Search Results ##############################
results_df <- findResultant_DF(plots =  plot_summaries, query = processedQuery)
resultDF <- merge(results_df, movieNames, by = "doc")
names(resultDF) <- c("doc", "score")
printDF <- resultDF[order(resultDF$score, decreasing = TRUE), ]
printDF <- printDF[,c(1,2,4)]
print()
names(printDF) <- c("doc", "score", "Movie Title")
if(is.na(printDF$score[1])) {
print("Sorry, there are no matches for your query. Please try another one.")
} else {
print(printDF[0:10,], row.names = FALSE, right = FALSE, digits = 2)
}
input <- readline(prompt = "Press 'q' to exit the program OR Press any other key to continue ")
}
library(SnowballC)
library(wordcloud)
library(text2vec)
library(dplyr)
library(abind)
############################## Preprocess the text data ##############################
preprocess <- function(text){
corpus <- Corpus(VectorSource(text))
uniqueWords = content_transformer(function(x) { unique(strsplit(x, " ")[[1]]) })
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c("a",
"about","above","after","again","against","all","am","an","and","any","are","aren't","as","at","be",
"because","been","before","being","below","between","both","but","by","can't","cannot","could","couldn't",
"did","didn't","do","does","doesn't","doing","don't","down","during","each","few","for","from","further",
"had","hadn't","has","hasn't","have","haven't","having","he","he'd","he'll","he's","her","here","here's",
"hers","herself","him","himself","his","how","how's","i","i'd","i'll","i'm","i've","if","in","into","is",
"isn't","it","it's","its","itself","let's","me","more","most","mustn't","my","myself","no","nor","not",
"of","off","on","once","only","or","other","ought","our","ours","ourselves","out","over","own","same",
"shan't","she","she'd","she'll","she's","should","shouldn't","so","some","such","than","that","that's",
"the","their","theirs","them","themselves","then","there","there's","these","they","they'd","they'll",
"they're","they've","this","those","through","to","too","under","until","up","very","was","wasn't","we",
"we'd","we'll","we're","we've","were","weren't","what","what's","when","when's","where","where's","which",
"while","who","who's","whom","why","why's","with","won't","would",
"wouldn't","you","you'd","you'll","you're","you've","your","yours","yourself","yourselves"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
corpus[[1]]$content
}
############################## TF-IDF ##############################
get_TF_IDF <- function(plots, query) {
cat("Calculating tf-idf values .......")
dtm1 <- DocumentTermMatrix(plots, query)
matrix <- t(apply(dtm1, 1,
FUN = function(row) {TF_IDF_Weights(row)}))
colnames(matrix) <- colnames(dtm1)
matrix <- scale(matrix, center = FALSE, scale = sqrt(colSums(matrix^2)))
document_count <- length(plots$summary)
query_vector <- matrix[, document_count + 1]
matrix <- matrix[, 1:document_count]
document_scores <- t(query_vector) %*% matrix
data.frame(doc = plots$doc_id, score = t(document_scores))
}
# Function to get term document weights for each document
TF_IDF_Weights <- function(tfvectors) {
document_count <- length(tfvectors)
document_frequency <- length(tfvectors[tfvectors > 0])
tfidf_weights <- rep(0, length(tfvectors))
tfidf_weights[tfvectors > 0] <- tfvectors[tfvectors > 0] * log2(document_count/document_frequency)
return(tfidf_weights)
}
findResultant_DF <- function(plots, query) {
query_word_count <- sapply(strsplit(query, " "), length)
if(query_word_count == 0) {
stop("Query cannot be NULL")
} else if (query_word_count == 1) {
get_TF_IDF(plots =  plots, query = query)
} else {
findCosineSimilarity(plots =  plots, query = query)
}
}
DocumentTermMatrix <- function(plots, query) {
document <- VectorSource(c(plots$summary, query))
document$names <- c(plots$doc_id, "query")
corpus <- Corpus(document)
corpus_dtm <- TermDocumentMatrix(corpus)
colnames(corpus_dtm) <- c(plots$doc_id, "query")
#Removing unnecessary elements as matrix is too sparse
sparse_corpus <- removeSparseTerms(corpus_dtm, 0.99)
as.matrix(sparse_corpus)
}
############################## COSINE Similarity ##############################
findCosineSimilarity <- function(plots, query) {
cat("Calculating Cosine Similarities -->")
documents <- plots$summary
iterator1 <- itoken(c(documents, query), progressbar = FALSE)
vocab = create_vocabulary(iterator1) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 3)
vectorizer = vocab_vectorizer(vocab)
iterator2 <- itoken(documents, progressbar = FALSE )
dtm1 = create_dtm(iterator2, vectorizer)
iterator3 <- itoken(query, progressbar = FALSE)
dtm2 = create_dtm(iterator3, vectorizer)
cosineSim = sim2(dtm1, dtm2, method = "cosine", norm = "l2")
cosine_matrix <- as.matrix(cosineSim)
data.frame(doc = plots$doc_id, score = cosine_matrix)
}
############################## Get Data ##############################
#df1 <- read.table("http://utdallas.edu/~dxm172530/R/plot_summaries.txt", header = FALSE,
#                  sep="\t", stringsAsFactors = FALSE, quote="")
#movieNames <- read.csv("http://utdallas.edu/~dxm172530/R/movie.metadata.tsv", sep = "\t", header = FALSE)
#colnames(movieNames) <- c("doc", "FreebaseID", "MovieName", "release date", "revenue",
#                          "runtime", "languages", "countries", "genres")
#Preprocess the data :
plot_summaries <- data.frame(doc_id = df1[,1], summary = df1[,2])
summaries <- lapply(plot_summaries$summary, as.character)
plot_summaries$summary <- lapply(summaries, preprocess)
plot_summaries <- data.frame(lapply(plot_summaries, as.character), stringsAsFactors = FALSE)
input <- ''
while(input != 'q') {
query <- readline(prompt="Enter your query: ")
processedQuery <- preprocess(query)
############################## Most frequent terms ##############################
dt_matrix <- DocumentTermMatrix(plots =  plot_summaries, query = processedQuery)
frequent_matrix<- addmargins(dt_matrix, margin = 2)
frequent_matrix<- frequent_matrix[order(frequent_matrix[,15], decreasing = TRUE),]
top_fifty_frequentmatrix<- head(frequent_matrix[,c(1,15)], n = 50)
print(top_fifty_frequentmatrix)
######################### Word Cloud ####################################
wordTable<- head(frequent_matrix[,15], n = 150)
wordcloud(words = names(wordTable),
freq = as.numeric(wordTable),
scale = c(2, 1),
min.freq = 1)
############################## Search Results ##############################
results_df <- findResultant_DF(plots =  plot_summaries, query = processedQuery)
resultDF <- merge(results_df, movieNames, by = "doc")
names(resultDF) <- c("doc", "score")
printDF <- resultDF[order(resultDF$score, decreasing = TRUE), ]
printDF <- printDF[,c(1,2,4)]
print()
names(printDF) <- c("doc", "score", "Movie Title")
if(is.na(printDF$score[1])) {
print("Sorry, there are no matches for your query. Please try another one.")
} else {
print(printDF[0:10,], row.names = FALSE, right = FALSE, digits = 2)
}
input <- readline(prompt = "Press 'q' to exit the program OR Press any other key to continue ")
}
library(SnowballC)
library(wordcloud)
library(text2vec)
library(dplyr)
library(abind)
############################## Preprocess the text data ##############################
preprocess <- function(text){
corpus <- Corpus(VectorSource(text))
uniqueWords = content_transformer(function(x) { unique(strsplit(x, " ")[[1]]) })
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, c("a",
"about","above","after","again","against","all","am","an","and","any","are","aren't","as","at","be",
"because","been","before","being","below","between","both","but","by","can't","cannot","could","couldn't",
"did","didn't","do","does","doesn't","doing","don't","down","during","each","few","for","from","further",
"had","hadn't","has","hasn't","have","haven't","having","he","he'd","he'll","he's","her","here","here's",
"hers","herself","him","himself","his","how","how's","i","i'd","i'll","i'm","i've","if","in","into","is",
"isn't","it","it's","its","itself","let's","me","more","most","mustn't","my","myself","no","nor","not",
"of","off","on","once","only","or","other","ought","our","ours","ourselves","out","over","own","same",
"shan't","she","she'd","she'll","she's","should","shouldn't","so","some","such","than","that","that's",
"the","their","theirs","them","themselves","then","there","there's","these","they","they'd","they'll",
"they're","they've","this","those","through","to","too","under","until","up","very","was","wasn't","we",
"we'd","we'll","we're","we've","were","weren't","what","what's","when","when's","where","where's","which",
"while","who","who's","whom","why","why's","with","won't","would",
"wouldn't","you","you'd","you'll","you're","you've","your","yours","yourself","yourselves"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
corpus[[1]]$content
}
############################## TF-IDF ##############################
get_TF_IDF <- function(plots, query) {
cat("Calculating tf-idf values -->")
dtm1 <- DocumentTermMatrix(plots, query)
matrix <- t(apply(dtm1, 1,
FUN = function(row) {TF_IDF_Weights(row)}))
colnames(matrix) <- colnames(dtm1)
matrix <- scale(matrix, center = FALSE, scale = sqrt(colSums(matrix^2)))
document_count <- length(plots$summary)
query_vector <- matrix[, document_count + 1]
matrix <- matrix[, 1:document_count]
document_scores <- t(query_vector) %*% matrix
data.frame(doc = plots$doc_id, score = t(document_scores))
}
# Function to get term document weights for each document
TF_IDF_Weights <- function(tfvectors) {
document_count <- length(tfvectors)
document_frequency <- length(tfvectors[tfvectors > 0])
tfidf_weights <- rep(0, length(tfvectors))
tfidf_weights[tfvectors > 0] <- tfvectors[tfvectors > 0] * log2(document_count/document_frequency)
return(tfidf_weights)
}
findResultant_DF <- function(plots, query) {
query_word_count <- sapply(strsplit(query, " "), length)
if(query_word_count == 0) {
stop("Query cannot be NULL")
} else if (query_word_count == 1) {
get_TF_IDF(plots =  plots, query = query)
} else {
findCosineSimilarity(plots =  plots, query = query)
}
}
DocumentTermMatrix <- function(plots, query) {
document <- VectorSource(c(plots$summary, query))
document$names <- c(plots$doc_id, "query")
corpus <- Corpus(document)
corpus_dtm <- TermDocumentMatrix(corpus)
colnames(corpus_dtm) <- c(plots$doc_id, "query")
#Removing unnecessary elements as matrix is too sparse
sparse_corpus <- removeSparseTerms(corpus_dtm, 0.99)
as.matrix(sparse_corpus)
}
############################## COSINE Similarity ##############################
findCosineSimilarity <- function(plots, query) {
cat("Calculating Cosine Similarities -->")
documents <- plots$summary
iterator1 <- itoken(c(documents, query), progressbar = FALSE)
vocab = create_vocabulary(iterator1) %>% prune_vocabulary(doc_proportion_max = 0.1, term_count_min = 3)
vectorizer = vocab_vectorizer(vocab)
iterator2 <- itoken(documents, progressbar = FALSE )
dtm1 = create_dtm(iterator2, vectorizer)
iterator3 <- itoken(query, progressbar = FALSE)
dtm2 = create_dtm(iterator3, vectorizer)
cosineSim = sim2(dtm1, dtm2, method = "cosine", norm = "l2")
cosine_matrix <- as.matrix(cosineSim)
data.frame(doc = plots$doc_id, score = cosine_matrix)
}
############################## Get Data ##############################
df1 <- read.table("http://utdallas.edu/~dxm172530/R/plot_summaries.txt", header = FALSE,
sep="\t", stringsAsFactors = FALSE, quote="")
movieNames <- read.csv("http://utdallas.edu/~dxm172530/R/movie.metadata.tsv", sep = "\t", header = FALSE)
colnames(movieNames) <- c("doc", "FreebaseID", "MovieName", "release date", "revenue",
"runtime", "languages", "countries", "genres")
#Preprocess the data :
plot_summaries <- data.frame(doc_id = df1[,1], summary = df1[,2])
summaries <- lapply(plot_summaries$summary, as.character)
plot_summaries$summary <- lapply(summaries, preprocess)
plot_summaries <- data.frame(lapply(plot_summaries, as.character), stringsAsFactors = FALSE)
input <- ''
while(input != 'q') {
query <- readline(prompt="Enter your query: ")
processedQuery <- preprocess(query)
############################## Most frequent terms ##############################
dt_matrix <- DocumentTermMatrix(plots =  plot_summaries, query = processedQuery)
frequent_matrix<- addmargins(dt_matrix, margin = 2)
frequent_matrix<- frequent_matrix[order(frequent_matrix[,15], decreasing = TRUE),]
top_fifty_frequentmatrix<- head(frequent_matrix[,c(1,15)], n = 50)
print(top_fifty_frequentmatrix)
######################### Word Cloud ####################################
wordTable<- head(frequent_matrix[,15], n = 150)
wordcloud(words = names(wordTable),
freq = as.numeric(wordTable),
scale = c(2, 1),
min.freq = 1)
############################## Search Results ##############################
results_df <- findResultant_DF(plots =  plot_summaries, query = processedQuery)
resultDF <- merge(results_df, movieNames, by = "doc")
names(resultDF) <- c("doc", "score")
printDF <- resultDF[order(resultDF$score, decreasing = TRUE), ]
printDF <- printDF[,c(1,2,4)]
print()
names(printDF) <- c("doc", "score", "Movie Title")
if(is.na(printDF$score[1])) {
print("Sorry, there are no matches for your query. Please try another one.")
} else {
print(printDF[0:10,], row.names = FALSE, right = FALSE, digits = 2)
}
input <- readline(prompt = "Press 'q' to exit the program OR Press any other key to continue ")
}
